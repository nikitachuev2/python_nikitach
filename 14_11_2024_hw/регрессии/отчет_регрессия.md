
В данном исследовании была проведена оценка зависимости целевой переменной от нескольких признаков с использованием различных моделей линейной и полиномиальной регрессии. Цель заключалась в том, чтобы выявить, какая из моделей наилучшим образом описывает имеющиеся данные.



Данные были загружены из CSV-файлов, содержащих признаки (features) и целевую переменную (targets). В результате первоначального анализа данные были выведены на экран, что позволило убедиться в их корректности.

- Признаки (Features): 
    - Включают в себя несколько переменных, представленных в виде числовых значений.

- Целевая переменная (Targets): 
    - Представляет значения, которые мы стремимся предсказать.



Для анализа были применены следующие методы регрессии:

1. Линейная регрессия:
   - Каждое из значений признаков было оценено отдельно с помощью простой линейной регрессии.
   - Кроме того, была проведена множественная линейная регрессия, учитывающая влияние всех признаков одновременно.

2. Полиномиальная регрессия:
   - Были построены полиномиальные модели регрессии, как 2-й, так и 3-й степени, для выявления связи между признаками и целевой переменной, учитывающей не линейные зависимости.



Для оценки качества моделей использовались следующие метрики:

- Среднеквадратичная ошибка (MSE): 
  Эта метрика показывает среднее значение квадратов ошибок (разница между предсказанными и фактическими значениями). Чем меньше значение MSE, тем лучше модель описывает данные.

- Коэффициент детерминации (R²): 
  Данная метрика измеряет, какую долю дисперсии целевой переменной удается объяснить моделью. Значение R² находится в пределах [0, 1], где значение, близкое к 1, указывает на хорошую подгонку модели.



После проведения анализа и обучения разных моделей, были получены следующие метрики:

- Линейные модели:
    - Для каждого признака, а также для комбинированной модели, были рассчитаны значения MSE и R².

- Полиномиальные модели:
    - Модели 2-й и 3-й степени также были протестированы на всех признаках, и их результаты записаны.

Полученные результаты метрик можно вывести в следующем виде:

| Модель                            | MSE        | R²         |
|-----------------------------------|------------|------------|
| Признак A                         | 7879.87    | 0.0247     |
| Признак B                         | 2340.99    | 0.7103     |
| Признак C                         | 4811.99    | 0.4044     |
| Комбинированная модель            | 101.37     | 0.9875     |
| Признак A^2                       | 10178.15   | -0.2598    |
| Признак B^2                       | 2298.75    | 0.7155     |
| Признак C^2                       | 5150.83    | 0.3625     |
| Признак A^3                       | 11231.32   | -0.3901    |
| Признак B^3                       | 2301.52    | 0.7151     |
| Признак C^3                       | 5695.78    | 0.2950     |
| Комбинированная^2                 | 99.01      | 0.9877     |


На основании полученных результатов, можно сделать вывод, что комбинация всех признаков в модели множественной линейной регрессии показала наилучшие результаты как по MSE (101.37), так и по R² (0.9875). Это говорит о том, что данная модель объясняет почти 99% вариации в целевой переменной.

Кроме того, полиномиальная регрессия 2-й степени также продемонстрировала хорошую подгонку.

В результате проведенного анализа выяснили, что множественная линейная регрессия была более эффективной в описании данных, чем простая линейная или полиномиальные модели. 